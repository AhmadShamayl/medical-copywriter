***DATA COLLECTION***
File Storage 


***CLEANING DATA***
with using re
made function to parse pdfs with these priorities:
marker > pdfplumber > PyPDF2

***CHUNKING***
Used Semantic Chunking Strategy 
Used Qwen 8b - It was a big model
then used Qwen 0.6B - Still It was a big model for my System
then used google's API for google-text-embedding-004 

***VECTOR-DB***
Implemented Chroma DB

***RETRIEVING***
Retrieve medical data from NCBI PubMed through their API
Retireve web search Data using Tavily API
The Json Structure for getting data from these APIs are same for langgraph Agent


***MCP***
MCP = hub layer between retrievers and your RAG agent.
Each retriever (Chroma, PubMed, Web) → registers as a “tool”.
MCP normalizes responses into one schema → so your LangGraph agent can consume them without worrying about source differences.